basic {
  data_dir = ../../data/  # Edit this
  download_dir = ${basic.data_dir} # dir that contains downloaded dataset
  log_root = ${basic.data_dir}
}

#*************** Dataset-specific config ***************
dataset = ${basic} {
  max_segment_len = 384
  doc_stride = 128
  max_query_len = 64
  max_answer_len = 30
  n_best_predictions = 10
  version_2_with_negative = False  # Only true for squad 2
  null_score_diff_threshold = 0.0
}

#*************** Model-specific config ***************

model {
  # Learning
  num_epochs = 4
  batch_size = 4
  bert_learning_rate = 3e-5
  adam_eps = 1e-8
  adam_weight_decay = 1e-4
  warmup_ratio = 0.1
  # warmup_step = 50
  max_grad_norm = 1
  gradient_accumulation_steps = 1

  # Other
  eval_frequency = 50
  report_frequency = 50

  train_dataset = tydiqa

  freeze_layer = 3
  k_spt=6
  q_qry=6
  meta_example_num = 1
  meta_steps = 4
  meta_learning_rate = 3e-5
  batch_size_meta = 1
  batch_sz = 500

  n_layer=2
  middle_size=768
  output_size=768
  mlp_dropout=0
  margin=0.5
  opt_type=adam
  learning_rate = 1e-4
  gclip=None
  path_ckpt = ../../output/sdmm/mbert_norm/best.ckpt
}

mbert_base = ${model}{
  model_type = mbert
  hidden_size = 768
  pretrained = bert-base-multilingual-cased
}

xlmr_base = ${model}{
  model_type = xlmr
  hidden_size = 768
  pretrained = xlm-roberta-base
}

xlmr1_base = ${model}{
  model_type = xlmr_tydi
  hidden_size = 768
  pretrained = xlm-roberta-base
}

xlmr_large = ${model}{
  model_type = xlmr_large
  hidden_size = 1024
  pretrained = xlm-roberta-large
}

xlm_100 = ${model}{
  model_type = xlm
  hidden_size = 1280
  pretrained = xlm-mlm-100-1280
}

#*************** Experiment-specific config ***************

mbert_zero_shot = ${dataset} ${mbert_base} {
  zero_shot = true
}

xlmr_zero_shot = ${dataset} ${xlmr_base} {
  zero_shot = true
}

xlmr_tydi_zero_shot = ${dataset} ${xlmr1_base} {
  zero_shot = true
}

xlmr_large_zero_shot = ${dataset} ${xlmr_large} {
  zero_shot = true
}

xlm_zero_shot = ${dataset} ${xlm_100} {
  zero_shot = true
}
